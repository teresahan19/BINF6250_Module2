{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#!/usr/bin/env python\n",
    "# Shebang line to execute script directly from command line\n",
    "\n",
    "# Import necessary packages/modules\n",
    "import random  # for generating random numbers in weighted_choice function\n",
    "import re  # for regex-based tokenization of text as states\n",
    "import textwrap  # for readability in generated text output to terminal and file\n",
    "\n",
    "# defaultdict to auto-initialize nested dictionaries for ngram counts\n",
    "# Counter to efficiently tally token frequencies\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "def read_text_file(filepath):\n",
    "    \"\"\"\n",
    "    Function to open and read input text file line by line, remove whitespace,\n",
    "    newline characters, and leading/trailing spaces, and return continuous\n",
    "    cleaned string of text\n",
    "        Parameters:\n",
    "            filepath (string): Path to input file\n",
    "        Returns:\n",
    "            text (string): Single continuous string containing properly formatted text\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize empty string to store processed text\n",
    "    text = \" \"\n",
    "\n",
    "    # Open file with context manager\n",
    "    with open(filepath, \"r\") as file:\n",
    "\n",
    "        # Iterate through file by line\n",
    "        for line in file:\n",
    "\n",
    "            # Strip leading/trailing whitespace and newline characters\n",
    "            text += \" \" + line.strip()  # Add space before lines for line distinction\n",
    "\n",
    "    # Return text string\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Helper function, converts raw text into lowercase tokens list\n",
    "    Tokenizes words, contractions, hyphens, numbers, punctuation\n",
    "        Parameters:\n",
    "            text (string): Raw text to tokenize\n",
    "        Returns:\n",
    "            tokens (list of strings): List of extracted tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # Regex pattern to capture: alphabetic words, words with hyphens or apostrophes,\n",
    "    # numbers, punctuations as individual tokens\n",
    "    pattern = r\"[A-Za-z]+(?:[-'][A-Za-z]+)*|\\d+|[\\\"'()\\-\\.,!?;:]\"\n",
    "\n",
    "    # Convert text to lowercase, extract tokens with regex\n",
    "    tokens = re.findall(pattern, text.lower())\n",
    "\n",
    "    # Return tokens as list of strings\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def word_frequencies(text):\n",
    "    \"\"\"\n",
    "    Count times each token exists in input text\n",
    "        Parameters:\n",
    "            text (string): Raw input text\n",
    "        Returns:\n",
    "            freqs (Counter): Mapping of tokens to frequencies\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert raw text into list of tokens\n",
    "    tokens = tokenize(text)\n",
    "\n",
    "    # Counter auto-tallies each token occurrence\n",
    "    freqs = Counter(tokens)\n",
    "\n",
    "    # Return frequency dictionary\n",
    "    return freqs\n",
    "\n",
    "\n",
    "def word_probabilities(text):\n",
    "    \"\"\"\n",
    "    Compute probability of tokens in text based on frequency of occurrence\n",
    "        Parameters:\n",
    "            text (string): Raw input text\n",
    "        Returns:\n",
    "            probs (dict): Mapping of tokens to probability values\n",
    "    \"\"\"\n",
    "\n",
    "    # Get raw frequency counts\n",
    "    freqs = word_frequencies(text)\n",
    "\n",
    "    # Compute total number tokens\n",
    "    total = sum(freqs.values())\n",
    "\n",
    "    # Convert counts to probabilities\n",
    "    probs = {token: count / total for token, count in freqs.items()}\n",
    "\n",
    "    # Return probability dictionary\n",
    "    return probs\n",
    "\n",
    "\n",
    "def ngram_frequencies(text, order):\n",
    "    \"\"\"\n",
    "    Count how often each n-gram state transitions to another token.\n",
    "    Establishes structure of Markov chain by mapping\n",
    "    states (tuple of length 'order') to dictionary of next-token counts\n",
    "        Parameters:\n",
    "            text (string): Raw input text\n",
    "            order (int): Markov chain order (N)\n",
    "        Returns:\n",
    "            freqs (dict): Mapping of state tuples to dictionaries of next-token counts\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize raw text into tokens list\n",
    "    tokens = tokenize(text)\n",
    "\n",
    "    # Add start-state (beta) \"*S*\" repeated 'order' times with one end marker \"*E*\"\n",
    "    tokens = [\"*S*\"] * order + tokens + [\"*E*\"]\n",
    "\n",
    "    # Initialize nested dictionary:\n",
    "    # outer dict: state tuple -> inner dict\n",
    "    # inner dict: next_token -> count\n",
    "    freqs = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Iterate through tokens to build ngram transitions\n",
    "    for i in range(len(tokens) - order):\n",
    "\n",
    "        # Extract state as tuple of previous 'order' tokens\n",
    "        state = tuple(tokens[i : i + order])\n",
    "\n",
    "        # Identify next token from current state\n",
    "        next_token = tokens[i + order]\n",
    "\n",
    "        # Increment count for transition\n",
    "        freqs[state][next_token] += 1\n",
    "\n",
    "    # Convert nested defaultdicts to normal dicts for clarity\n",
    "    return {state: dict(next_tokens) for state, next_tokens in freqs.items()}\n",
    "\n",
    "\n",
    "def build_markov_model(text, order):\n",
    "    \"\"\"\n",
    "    Build Markov model from input text by computing ngram frequencies to generate\n",
    "    text based on tokens from input text\n",
    "        Parameters:\n",
    "            text (string): Raw input text\n",
    "            order (int): Markov chain order (N)\n",
    "        Returns:\n",
    "            model (dict): Mapping of state tuples to next-token counts\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute ngram transition frequencies\n",
    "    model = ngram_frequencies(text, order)\n",
    "\n",
    "    # Return model dictionary\n",
    "    return model\n",
    "\n",
    "\n",
    "def distribution_frequencies(items):\n",
    "    \"\"\"\n",
    "    Count frequencies of any text states\n",
    "        Parameters:\n",
    "            items (list): List of hashable items\n",
    "        Returns:\n",
    "            freqs (dict): Mapping of items to associated frequencies\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionary to track occurrences\n",
    "    freqs = defaultdict(int)\n",
    "\n",
    "    # Count each item\n",
    "    for item in items:\n",
    "        freqs[item] += 1\n",
    "\n",
    "    # Return normal dictionary\n",
    "    return dict(freqs)\n",
    "\n",
    "\n",
    "def distribution_probabilities(freqs):\n",
    "    \"\"\"\n",
    "    Convert frequency dictionary into probability distribution\n",
    "        Parameters:\n",
    "            freqs (dict): Mapping of items to frequency counts\n",
    "        Returns:\n",
    "            probs (dict): Mapping of items to probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute total count\n",
    "    total = sum(freqs.values())\n",
    "\n",
    "    # Initialize probability dictionary\n",
    "    probs = {}\n",
    "\n",
    "    # Convert counts into probabilities\n",
    "    for item, count in freqs.items():\n",
    "        if total == 0:\n",
    "            probs[item] = 0.0\n",
    "        else:\n",
    "            probs[item] = count / total\n",
    "\n",
    "    # Return probability dictionary\n",
    "    return probs\n",
    "\n",
    "\n",
    "def beta_frequencies(text, order):\n",
    "    \"\"\"\n",
    "    Count number of times starting state appears in text.\n",
    "    For Nth-order Markov chains start state is first 'order' tokens\n",
    "        Parameters:\n",
    "            text (string): Raw input text\n",
    "            order (int): Markov chain order (N)\n",
    "        Returns:\n",
    "            freqs (dict): Mapping of start-state tuples to frequencies\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize raw text\n",
    "    tokens = tokenize(text)\n",
    "\n",
    "    # Add start markers to simulate missing prior context\n",
    "    tokens = [\"*S*\"] * order + tokens\n",
    "\n",
    "    # Extract start state as tuple of first 'order' tokens\n",
    "    start_state = tuple(tokens[:order])\n",
    "\n",
    "    # Count start state\n",
    "    freqs = distribution_frequencies([start_state])\n",
    "\n",
    "    # Return frequency dictionary\n",
    "    return freqs\n",
    "\n",
    "\n",
    "def beta_probabilities(beta_freqs):\n",
    "    \"\"\"\n",
    "    Convert start-state frequencies (beta) into start-state probabilities\n",
    "        Parameters:\n",
    "            beta_freqs (dict): Mapping of start-state tuples to frequencies\n",
    "        Returns:\n",
    "            beta_probs (dict): Mapping of start-state tuples to probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    # Transform frequencies into probabilities\n",
    "    beta_probs = distribution_probabilities(beta_freqs)\n",
    "\n",
    "    # Return probability dictionary\n",
    "    return beta_probs\n",
    "\n",
    "\n",
    "def omega_frequencies(markov_model):\n",
    "    \"\"\"\n",
    "    Count how many times states transition to end marker \"*E*\"\n",
    "        Parameters:\n",
    "            markov_model (dict): Mapping of state tuples to next-token tallies\n",
    "        Returns:\n",
    "            omega_freqs (dict): Mapping of state tuples to ending frequencies\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionary for ending frequencies\n",
    "    omega_freqs = {}\n",
    "\n",
    "    # Iterate through all states in model\n",
    "    for state, transitions in markov_model.items():\n",
    "\n",
    "        # If state transitions to \"*E*\", record count\n",
    "        if \"*E*\" in transitions:\n",
    "            omega_freqs[state] = transitions[\"*E*\"]\n",
    "        else:\n",
    "            # Otherwise, ending frequency is zero\n",
    "            omega_freqs[state] = 0\n",
    "\n",
    "    # Return dictionary of ending frequencies\n",
    "    return omega_freqs\n",
    "\n",
    "\n",
    "def omega_probabilities(omega_freqs):\n",
    "    \"\"\"\n",
    "    Convert end-state frequencies (omega) into end-state probabilities\n",
    "        Parameters:\n",
    "            omega_freqs (dict): Mapping of state tuples to end-state frequencies\n",
    "        Returns:\n",
    "            omega_probs (dict): Mapping of state tuples to end probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert ending frequencies into probabilities\n",
    "    omega_probs = distribution_probabilities(omega_freqs)\n",
    "\n",
    "    # Return probability dictionary\n",
    "    return omega_probs\n",
    "\n",
    "\n",
    "def weighted_choice(options_dict):\n",
    "    \"\"\"\n",
    "    Select token from dictionary of {token: probability} by applying\n",
    "    cumulative probability weighting\n",
    "        Parameters:\n",
    "            options_dict (dict): Mapping of tokens to probability values\n",
    "        Returns:\n",
    "            token (string): Selected token based on weighted random choice\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate random float between 0 and 1\n",
    "    random_float = random.random()\n",
    "\n",
    "    # Track cumulative probability\n",
    "    cumulative = 0.0\n",
    "\n",
    "    # Iterate through tokens and their probabilities\n",
    "    for token, prob in options_dict.items():\n",
    "\n",
    "        # Add probability to cumulative total\n",
    "        cumulative += prob\n",
    "\n",
    "        # If random number falls within cumulative range, return token\n",
    "        if random_float <= cumulative:\n",
    "            return token\n",
    "\n",
    "    # Fallback return\n",
    "    return token\n",
    "\n",
    "\n",
    "def get_next_word(state, model, omega_probs):\n",
    "    \"\"\"\n",
    "    Given current state, predict next token based on transition probabs from Markov model,\n",
    "    ending probabilities from omega\n",
    "        Parameters:\n",
    "            state (tuple): Current Markov state of length: order\n",
    "            model (dict): Mapping of states to next-token counts\n",
    "            omega_probs (dict): End-state probability dist\n",
    "        Returns:\n",
    "            next_token (string): Predicted next token\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve ending probability for state (default 0.0)\n",
    "    end_prob = omega_probs.get(state, 0.0)\n",
    "\n",
    "    # Initialize dictionary for next-token probabilities\n",
    "    next_token_probs = {}\n",
    "\n",
    "    # If state exists, compute transition probs\n",
    "    if state in model:\n",
    "\n",
    "        # Retrieve next-token counts\n",
    "        transitions = model[state]\n",
    "\n",
    "        # Compute total transitions from current state\n",
    "        total = sum(transitions.values())\n",
    "\n",
    "        # Convert counts to probs\n",
    "        for token, count in transitions.items():\n",
    "            next_token_probs[token] = count / total\n",
    "\n",
    "    # Add ending prob as possible next token\n",
    "    if end_prob > 0:\n",
    "        next_token_probs[\"*E*\"] = end_prob\n",
    "\n",
    "    # Select next token using weighted random choice\n",
    "    return weighted_choice(next_token_probs)\n",
    "\n",
    "\n",
    "def compute_metadata(text):\n",
    "    \"\"\"\n",
    "    Optional helper to compute metadata, incl average sentence length\n",
    "        Parameters:\n",
    "            text (string): Raw input text\n",
    "        Returns:\n",
    "            metadata (dict): Contains average sentence length\n",
    "    \"\"\"\n",
    "\n",
    "    # Split text into sentences using punctuation\n",
    "    sentences = re.split(r\"[.!?]+\", text)\n",
    "\n",
    "    # Remove empty strings and strip whitespace\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    # If no sentences found, return default metadata\n",
    "    if not sentences:\n",
    "        return {\"avg_length\": 45}\n",
    "\n",
    "    # Compute number of words per sentence\n",
    "    word_counts = [len(s.split()) for s in sentences]\n",
    "\n",
    "    # Compute average sentence length\n",
    "    avg_length = round(sum(word_counts) / len(word_counts))\n",
    "\n",
    "    # Return metadata dictionary\n",
    "    return {\"avg_length\": avg_length}\n",
    "\n",
    "\n",
    "def generate_random_text(model, beta_probs, omega_probs, order, max_words=150, metadata=None, use_metadata=False):\n",
    "    \"\"\"\n",
    "    Generate random text sequence from Markov model application to input text\n",
    "        Parameters:\n",
    "            model (dict): Mapping of states to next-token counts\n",
    "            beta_probs (dict): Start-state probability distribution\n",
    "            omega_probs (dict): Ending-state probability distribution\n",
    "            order (int): Markov chain order (N)\n",
    "            max_words (int): Maximum tokens to generate\n",
    "            metadata (dict): Optional metadata (avg sentence length)\n",
    "            use_metadata (bool): Whether to use metadata to guide length\n",
    "\n",
    "        Returns:\n",
    "            generated_text (string): Generated text sequence\n",
    "    \"\"\"\n",
    "\n",
    "    # Choose starting state using beta probabilities\n",
    "    current_state = weighted_choice(beta_probs)\n",
    "\n",
    "    # Initialize list to store generated tokens\n",
    "    generated = []\n",
    "\n",
    "    # Determine target length based on metadata or max_words\n",
    "    if use_metadata and metadata:\n",
    "        target_length = metadata[\"avg_length\"]\n",
    "    else:\n",
    "        target_length = max_words\n",
    "\n",
    "    # Generate up to target_length tokens\n",
    "    for _ in range(target_length):\n",
    "\n",
    "        # Predict next token\n",
    "        next_token = get_next_word(current_state, model, omega_probs)\n",
    "\n",
    "        # Stop if end marker reached\n",
    "        if next_token == \"*E*\":\n",
    "            break\n",
    "\n",
    "        # Append token to output list\n",
    "        generated.append(next_token)\n",
    "\n",
    "        # Update state by shifting left and adding new token\n",
    "        current_state = tuple(list(current_state[1:]) + [next_token])\n",
    "\n",
    "    # Join tokens into single string\n",
    "    return \" \".join(generated)\n",
    "\n",
    "\n",
    "def output_generated_text(text, filename, width=80):\n",
    "    \"\"\"\n",
    "    Print generated text to terminal and save to output file using text wrapping\n",
    "        Parameters:\n",
    "            text (string): Text generated from Markov model based on input text style\n",
    "            filename (string): Name of output file\n",
    "            width (int): Maximum characters per line for wrapping\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # Wrap text for readability\n",
    "    wrapped_text = textwrap.fill(text, width=width)\n",
    "\n",
    "    # Print wrapped text to terminal\n",
    "    print(wrapped_text)\n",
    "\n",
    "    # Save wrapped text to output file\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(wrapped_text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Driver code to run Markov chain text generator\n",
    "    \"\"\"\n",
    "\n",
    "    # Input text file from project directory\n",
    "    input_file = \"one_fish_two_fish.txt\"\n",
    "\n",
    "    # Markov chain order (N)\n",
    "    order = 8\n",
    "\n",
    "    # Read, prepare input text\n",
    "    raw_text = read_text_file(input_file)\n",
    "\n",
    "    # Build Markov model from input text\n",
    "    model = build_markov_model(raw_text, order)\n",
    "\n",
    "    # Compute beta (start-state) distribution\n",
    "    beta_freqs = beta_frequencies(raw_text, order)\n",
    "    beta_probs = beta_probabilities(beta_freqs)\n",
    "\n",
    "    # Compute omega (end-state) distribution\n",
    "    omega_freqs = omega_frequencies(model)\n",
    "    omega_probs = omega_probabilities(omega_freqs)\n",
    "\n",
    "    # Compute metadata (avg sentence length) if desired\n",
    "    metadata = compute_metadata(raw_text)\n",
    "\n",
    "    # Generate text from trained Markov model\n",
    "    generated_text = generate_random_text(\n",
    "        model=model,\n",
    "        beta_probs=beta_probs,\n",
    "        omega_probs=omega_probs,\n",
    "        order=order,\n",
    "        max_words=200,\n",
    "        metadata=metadata,\n",
    "        use_metadata=False\n",
    "    )\n",
    "\n",
    "    # Print and save generated text\n",
    "    output_generated_text(generated_text, \"generated_output.txt\")\n"
   ],
   "id": "2b1318a7b3a11c31"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
